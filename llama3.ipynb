{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA3(\n",
      "  (embedding): Embedding(50257, 768)\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-11): 12 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=50257, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA3(\n",
      "  (embedding): Embedding(50257, 768)\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-11): 12 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-11): 12 x TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=50257, bias=True)\n",
      ")\n",
      "Loss: 10.824667930603027\n",
      "tensor([[    0, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821,\n",
      "         33821, 33821, 33821, 33821, 33821, 33821, 33821, 33821]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LLaMA3(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_heads, num_layers, max_length, device):\n",
    "        super(LLaMA3, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_length, hidden_dim).to(device)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim*4) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim*4) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Typically the output dimension matches the vocab size for language models\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.02)\n",
    "        for module in self.encoder_layers:\n",
    "            self._init_layer_weights(module)\n",
    "        for module in self.decoder_layers:\n",
    "            self._init_layer_weights(module)\n",
    "        nn.init.normal_(self.fc.weight, mean=0, std=0.02)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "\n",
    "    def _init_layer_weights(self, layer):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param, mean=0, std=0.02)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_length, hidden_dim):\n",
    "        pe = torch.zeros(max_length, hidden_dim)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq_len = src.size(1)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        \n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src_seq_len, :]\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt_seq_len, :]\n",
    "        \n",
    "        src = src.transpose(0, 1)  # (seq_len, batch_size, hidden_dim)\n",
    "        tgt = tgt.transpose(0, 1)  # (seq_len, batch_size, hidden_dim)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src)\n",
    "        \n",
    "        logits = self.fc(tgt.transpose(0, 1))  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "vocab_size = 50257\n",
    "hidden_dim = 768\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "max_length = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LLaMA3(vocab_size, hidden_dim, num_heads, num_layers, max_length, device).to(device)\n",
    "print(model)\n",
    "\n",
    "# Training loop for Llama3\n",
    "model.train()\n",
    "\n",
    "# Using adam optimizer and cross entropy loss \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the input and target tensors\n",
    "batch_size = 32\n",
    "seq_length = 128\n",
    "src = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(src, tgt[:, :-1])\n",
    "\n",
    "# Computing the loss\n",
    "loss = criterion(logits.view(-1, vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('Loss:', loss.item())\n",
    "\n",
    "# model Evaluation\n",
    "model.eval()\n",
    "\n",
    "# Here we define the input tensor\n",
    "src = torch.randint(0, vocab_size, (1, 128), device=device)\n",
    "\n",
    "# Define the target tensor\n",
    "tgt = torch.zeros((1, 128), dtype=torch.long, device=device)\n",
    "\n",
    "# Initialize the first token as the start token\n",
    "tgt[0, 0] = 0\n",
    "\n",
    "# Generate the output sequence\n",
    "for i in range(1, 128):\n",
    "    logits = model(src, tgt[:, :i])\n",
    "    next_token = torch.argmax(logits[0, i - 1, :]).item()\n",
    "    tgt[0, i] = next_token\n",
    "\n",
    "print(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (4.62.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.722799613133001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 6.495773627415227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 85it [3:33:12, 52.78s/it, loss=6.36] "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "#pip install tqdm\n",
    "subprocess.run([\"pip\", \"install\", \"tqdm\"])\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for file_id in corpus.fileids():\n",
    "        words = word_tokenize(corpus.raw(file_id).lower())\n",
    "        tokens.extend(words)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_corpus(gutenberg)\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "def tokens_to_tensor(tokens, word_to_idx):\n",
    "    return torch.tensor([word_to_idx[token] for token in tokens if token in word_to_idx], dtype=torch.long)\n",
    "\n",
    "token_tensor = tokens_to_tensor(tokens, word_to_idx)\n",
    "\n",
    "hidden_dim = 768\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "max_length = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LLaMA3(vocab_size, hidden_dim, num_heads, num_layers, max_length, device).to(device)\n",
    "\n",
    "def create_batches(token_tensor, batch_size, seq_length):\n",
    "    num_batches = token_tensor.size(0) // (batch_size * seq_length)\n",
    "    data = token_tensor[:num_batches * batch_size * seq_length]\n",
    "    data = data.view(batch_size, -1)\n",
    "    for i in range(0, data.size(1) - seq_length, seq_length):\n",
    "        src = data[:, i:i+seq_length]\n",
    "        tgt = data[:, i+1:i+seq_length+1]\n",
    "        yield src, tgt\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 128\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(create_batches(token_tensor, batch_size, seq_length), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    for src, tgt in progress_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        logits = model(src, tgt[:, :-1])\n",
    "        loss = criterion(logits.view(-1, vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / (len(token_tensor) // (batch_size * seq_length))}')\n",
    "\n",
    "print('Training complete.')\n",
    "\n",
    "model.eval()\n",
    "src = torch.randint(0, vocab_size, (1, 128), device=device)\n",
    "tgt = torch.zeros((1, 128), dtype=torch.long, device=device)\n",
    "tgt[0, 0] = word_to_idx['<start>']  # Use the appropriate start token for your dataset\n",
    "\n",
    "for i in range(1, 128):\n",
    "    logits = model(src, tgt[:, :i])\n",
    "    next_token = torch.argmax(logits[0, i - 1, :]).item()\n",
    "    tgt[0, i] = next_token\n",
    "\n",
    "generated_text = ' '.join([idx_to_word[idx] for idx in tgt[0].tolist()])\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
