{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/krishpatel/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Epoch 1/10: 100%|██████████| 1585/1585 [15:40<00:00,  1.68it/s, loss=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6455237504522331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1585/1585 [15:04<00:00,  1.75it/s, loss=0.0783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.07858108892252323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1585/1585 [14:43<00:00,  1.79it/s, loss=0.00483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.008561585780004424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1585/1585 [15:23<00:00,  1.72it/s, loss=0.000132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.00010295823949548368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1585/1585 [15:00<00:00,  1.76it/s, loss=7.86e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 6.329442117290767e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1585/1585 [16:25<00:00,  1.61it/s, loss=4.26e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 3.620434094949537e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1585/1585 [17:49<00:00,  1.48it/s, loss=2.05e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.9007340314742674e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1585/1585 [15:39<00:00,  1.69it/s, loss=9.61e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 9.332292182625908e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1585/1585 [17:14<00:00,  1.53it/s, loss=4.33e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 4.396620280974653e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1585/1585 [18:23<00:00,  1.44it/s, loss=1.95e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 2.0408212613311005e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Load and tokenize the corpus\n",
    "corpus = gutenberg.raw()\n",
    "tokens = word_tokenize(corpus.lower())\n",
    "\n",
    "# Build vocabulary and mappings\n",
    "vocab = Counter(tokens)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Convert tokens to indices\n",
    "data = [word_to_idx[word] for word in tokens if word in word_to_idx]\n",
    "\n",
    "def create_batches(data, batch_size, seq_length):\n",
    "    num_batches = len(data) // (batch_size * seq_length)\n",
    "    data = data[:num_batches * batch_size * seq_length]\n",
    "    inputs = np.array(data).reshape((batch_size, -1))\n",
    "    targets = np.copy(inputs)\n",
    "    targets[:, :-1], targets[:, -1] = inputs[:, 1:], inputs[:, 0]\n",
    "    return inputs, targets\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "inputs, targets = create_batches(data, batch_size, seq_length)\n",
    "\n",
    "class miniLlama(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos):\n",
    "        super(miniLlama, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos = pos\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward), num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward), num_decoder_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        src = self.encoder(src)\n",
    "        tgt = self.decoder(tgt, src)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        tgt = self.fc(tgt)\n",
    "        return tgt\n",
    "    \n",
    "    def generate(self, src):\n",
    "        src = self.embedding(src)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.encoder(src)\n",
    "        tgt = torch.zeros((1, src.shape[1], self.d_model)).to(src.device)\n",
    "        for i in range(self.max_seq_length):\n",
    "            tgt = self.decoder(tgt, src)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = self.fc(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = torch.argmax(tgt, dim=2)\n",
    "            if torch.all(tgt[-1] == self.pos):\n",
    "                break\n",
    "        return tgt\n",
    "    \n",
    "    def generate_beam(self, src, beam_size):\n",
    "        src = self.embedding(src)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.encoder(src)\n",
    "        tgt = torch.zeros((1, src.shape[1], self.d_model)).to(src.device)\n",
    "        beam = torch.zeros((1, src.shape[1], self.d_model)).to(src.device)\n",
    "        for i in range(self.max_seq_length):\n",
    "            tgt = self.decoder(tgt, src)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = self.fc(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = torch.argmax(tgt, dim=2)\n",
    "            if torch.all(tgt[-1] == self.pos):\n",
    "                break\n",
    "        return tgt\n",
    "    \n",
    "    def generate_greedy(self, src):\n",
    "        src = self.embedding(src)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.encoder(src)\n",
    "        tgt = torch.zeros((1, src.shape[1], self.d_model)).to(src.device)\n",
    "        for i in range(self.max_seq_length):\n",
    "            tgt = self.decoder(tgt, src)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = self.fc(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = torch.argmax(tgt, dim=2)\n",
    "            if torch.all(tgt[-1] == self.pos):\n",
    "                break\n",
    "        return tgt\n",
    "\n",
    "# Training loop\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    vocab_size = len(vocab)\n",
    "    d_model = 128\n",
    "    nhead = 4\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    dim_feedforward = 512\n",
    "    max_seq_length = seq_length\n",
    "    pos = torch.tensor([vocab_size-1])\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = miniLlama(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(range(0, inputs.shape[1] - seq_length, seq_length), desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        for i in progress_bar:\n",
    "            input_batch = torch.tensor(inputs[:, i:i+seq_length], dtype=torch.long)\n",
    "            target_batch = torch.tensor(targets[:, i:i+seq_length], dtype=torch.long)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_batch, target_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / (inputs.shape[1] // seq_length)}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'mini_llama_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Sentence: The\n",
      "Generated Text: whole-length whitened whitened whitened whitened whitened crotchets sihon sihon sihon elysee sihon frog frog frog dec. frog frog frog dec. frog remembereth frog dec. frog remembereth frog coax frog coax frog coax frog coax frog frog coax frog coax frog coax frog coax frog frog coax frog coax frog coax\n",
      "\n",
      "Start Sentence: God\n",
      "Generated Text: whole-length whitened whitened whitened whitened crotchets inns sihon frog dec. frog frog frog dec. frog frog dec. vigil frog coax frog frog coax frog frog coax frog coax frog frog coax frog coax frog frog coax frog coax frog frog coax frog coax frog coax frog frog coax frog fallow\n",
      "\n",
      "Start Sentence: Bible\n",
      "Generated Text: whole-length whitened whitened whitened sihon sketch sihon sihon elysee whole-length whitened whitened whitened whitened whitened sihon sihon sihon sihon sihon whitened sihon elysee whitened sihon sketch whole-length whitened whitened whitened sihon sihon sihon sihon sihon elysee whitened whitened sihon sihon sihon sketch whole-length whole-length whitened whitened whitened whitened whitened whitened\n",
      "\n",
      "Start Sentence: Thou\n",
      "Generated Text: whole-length whitened whitened whitened whitened whitened crotchets inns sihon sihon elysee sihon sihon sihon elysee frog frog frog frog dec. frog frog frog dec. frog frog remembereth dec. frog remembereth frog dec. vigil frog coax frog coax frog coax frog frog coax frog coax frog coax frog coax frog frog\n",
      "\n",
      "Start Sentence: his\n",
      "Generated Text: whole-length whitened whitened whitened inns sihon dec. sihon jocularly sihon elysee frog frog frog dec. frog frog dec. frog vigil frog frog dec. coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax\n",
      "\n",
      "Start Sentence: she\n",
      "Generated Text: whole-length whitened whitened whitened crotchets inns sihon sihon elysee frog frog dec. frog frog dec. vigil shelf frog frog shelf frog shelf frog coax frog coax frog frog coax frog coax frog coax frog vigil coax frog coax frog frog coax frog coax frog coax frog coax frog frog coax\n",
      "\n",
      "Start Sentence: he\n",
      "Generated Text: whole-length whitened whitened whitened whitened crotchets inns frog dec. frog frog vigil frog dec. frog coax frog coax frog frog coax frog coax frog frog coax frog coax frog coax frog vigil frog coax frog coax frog coax frog frog coax frog coax frog coax frog frog coax frog coax\n",
      "\n",
      "Start Sentence: In a hidden cave,\n",
      "Generated Text: whole-length whitened whitened whitened inns shelf muskrat crotchets frog dec. frog frog frog dec. coax frog frog coax frog coax frog coax frog coax frog coax frog frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog frog coax frog coax\n",
      "\n",
      "Start Sentence: In the ancient castle,\n",
      "Generated Text: whole-length whitened whitened whitened whitened inns sihon dec. frog shelf frog frog dec. frog shelf frog coax frog frog coax frog coax frog coax frog coax frog coax frog frog coax frog coax frog coax frog coax frog coax frog coax frog coax frog frog coax fallow frog coax frog\n",
      "\n",
      "Start Sentence: On a distant planet,\n",
      "Generated Text: whole-length whitened whitened whitened whitened crotchets inns sihon sihon elysee sihon frog frog dec. frog frog dec. frog frog shelf frog dec. vigil frog shelf frog coax frog coax frog frog coax frog coax frog coax frog coax frog frog coax frog coax frog coax frog coax frog coax frog\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the hyperparameters\n",
    "d_model = 128\n",
    "nhead = 2\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 512\n",
    "max_seq_length = 50  # same as seq_length\n",
    "pos = torch.tensor([vocab_size-1])\n",
    "\n",
    "# Load the saved model weights\n",
    "model_path = 'mini_llama_model.pth'\n",
    "model = miniLlama(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "start_sentences = [\n",
    "    \"The\",\n",
    "    \"God\",\n",
    "    \"Bible\",\n",
    "    \"Thou\",\n",
    "    \"his\",\n",
    "    \"she\",\n",
    "    \"he\",\n",
    "    \"In a hidden cave,\",\n",
    "    \"In the ancient castle,\",\n",
    "    \"On a distant planet,\"\n",
    "]\n",
    "\n",
    "# Function to tokenize the start sentences\n",
    "def tokenize_start_sentence(sentence, word_to_idx):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    return torch.tensor([word_to_idx[token] for token in tokens if token in word_to_idx]).unsqueeze(0)\n",
    "\n",
    "# Adjusted generate method to handle dimensions properly\n",
    "def generate(self, src):\n",
    "    src = self.embedding(src)\n",
    "    src = src.permute(1, 0, 2)\n",
    "    src = self.encoder(src)\n",
    "    tgt = torch.zeros((self.max_seq_length, src.shape[1], self.d_model)).to(src.device)\n",
    "    for i in range(self.max_seq_length):\n",
    "        tgt_input = tgt[:i+1]\n",
    "        output = self.decoder(tgt_input, src)\n",
    "        output = self.fc(output)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        tgt[i] = self.embedding(output[-1])\n",
    "        if torch.all(output[-1] == self.pos):\n",
    "            break\n",
    "    return output\n",
    "\n",
    "# Attach the new generate method to the model\n",
    "miniLlama.generate = generate\n",
    "\n",
    "# Generate text for each start sentence\n",
    "model.eval()\n",
    "for sentence in start_sentences:\n",
    "    start_tokens = tokenize_start_sentence(sentence, word_to_idx)\n",
    "    generated_tokens = model.generate(start_tokens)\n",
    "    generated_sentence = ' '.join([idx_to_word[token.item()] for token in generated_tokens.squeeze()])\n",
    "    print(f\"Start Sentence: {sentence}\\nGenerated Text: {generated_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (4.62.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.722799613133001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 6.495773627415227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 123it [4:08:23, 57.43s/it, loss=6.47] "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "#pip install tqdm\n",
    "subprocess.run([\"pip\", \"install\", \"tqdm\"])\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = []\n",
    "    for file_id in corpus.fileids():\n",
    "        words = word_tokenize(corpus.raw(file_id).lower())\n",
    "        tokens.extend(words)\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_corpus(gutenberg)\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "def tokens_to_tensor(tokens, word_to_idx):\n",
    "    return torch.tensor([word_to_idx[token] for token in tokens if token in word_to_idx], dtype=torch.long)\n",
    "\n",
    "token_tensor = tokens_to_tensor(tokens, word_to_idx)\n",
    "\n",
    "hidden_dim = 768\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "max_length = 512\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LLaMA3(vocab_size, hidden_dim, num_heads, num_layers, max_length, device).to(device)\n",
    "\n",
    "def create_batches(token_tensor, batch_size, seq_length):\n",
    "    num_batches = token_tensor.size(0) // (batch_size * seq_length)\n",
    "    data = token_tensor[:num_batches * batch_size * seq_length]\n",
    "    data = data.view(batch_size, -1)\n",
    "    for i in range(0, data.size(1) - seq_length, seq_length):\n",
    "        src = data[:, i:i+seq_length]\n",
    "        tgt = data[:, i+1:i+seq_length+1]\n",
    "        yield src, tgt\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 128\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(create_batches(token_tensor, batch_size, seq_length), desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    for src, tgt in progress_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        logits = model(src, tgt[:, :-1])\n",
    "        loss = criterion(logits.view(-1, vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / (len(token_tensor) // (batch_size * seq_length))}')\n",
    "\n",
    "print('Training complete.')\n",
    "\n",
    "model.eval()\n",
    "src = torch.randint(0, vocab_size, (1, 128), device=device)\n",
    "tgt = torch.zeros((1, 128), dtype=torch.long, device=device)\n",
    "tgt[0, 0] = word_to_idx['<start>']  # Use the appropriate start token for your dataset\n",
    "\n",
    "for i in range(1, 128):\n",
    "    logits = model(src, tgt[:, :i])\n",
    "    next_token = torch.argmax(logits[0, i - 1, :]).item()\n",
    "    tgt[0, i] = next_token\n",
    "\n",
    "generated_text = ' '.join([idx_to_word[idx] for idx in tgt[0].tolist()])\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/krishpatel/anaconda3/lib/python3.11/site-packages/litellm-1.40.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/krishpatel/anaconda3/lib/python3.11/site-packages/openai-0.27.7-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (4.62.1)\n",
      "Requirement already satisfied: xxhash in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: filelock in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: six in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (20.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AbstractAsyncStreamedFile' from 'fsspec.asyn' (/Users/krishpatel/anaconda3/lib/python3.11/site-packages/fsspec/asyn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m     10\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m     14\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, concatenate_datasets\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio, ClassLabel, Features, Image, Sequence, Value\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureType, _ArrayXD, decode_nested_example, pandas_types_mapper, require_decoding\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/arrow_writer.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     FeatureType,\n\u001b[1;32m     29\u001b[0m     _ArrayXDExtensionType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     numpy_to_pyarrow_listarray,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/features/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray2D\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslationVariableLanguages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/features/audio.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_cast\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m no_op_if_value_is_null, string_to_dict\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xopen\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureType\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/streaming_download_manager.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maiohttp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPRESSION_FILESYSTEMS\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig, map_nested\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     get_authentication_headers_for_url,\n\u001b[1;32m     23\u001b[0m     http_head,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     url_or_path_join,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/filesystems/__init__.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m _has_s3fs \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3fs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_s3fs:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3filesystem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     15\u001b[0m COMPRESSION_FILESYSTEMS: List[compression\u001b[38;5;241m.\u001b[39mBaseCompressedFileFileSystem] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     compression\u001b[38;5;241m.\u001b[39mBz2FileSystem,\n\u001b[1;32m     17\u001b[0m     compression\u001b[38;5;241m.\u001b[39mGzipFileSystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     compression\u001b[38;5;241m.\u001b[39mZstdFileSystem,\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Register custom filesystems\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/filesystems/s3filesystem.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01ms3fs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mS3FileSystem\u001b[39;00m(s3fs\u001b[38;5;241m.\u001b[39mS3FileSystem):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    `datasets.filesystems.S3FileSystem` is a subclass of [`s3fs.S3FileSystem`](https://s3fs.readthedocs.io/en/latest/api.html).\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/s3fs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3FileSystem, S3File\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3Map\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/s3fs/core.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractBufferedFile\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_storage_options, tokenize, setup_logging \u001b[38;5;28;01mas\u001b[39;00m setup_logger\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     AsyncFileSystem,\n\u001b[1;32m     19\u001b[0m     AbstractAsyncStreamedFile,\n\u001b[1;32m     20\u001b[0m     sync,\n\u001b[1;32m     21\u001b[0m     sync_wrapper,\n\u001b[1;32m     22\u001b[0m     FSTimeoutError,\n\u001b[1;32m     23\u001b[0m     _run_coros_in_chunks,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _DEFAULT_CALLBACK\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiobotocore\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AbstractAsyncStreamedFile' from 'fsspec.asyn' (/Users/krishpatel/anaconda3/lib/python3.11/site-packages/fsspec/asyn.py)"
     ]
    }
   ],
   "source": [
    "# NEW CODE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"datasets\"])\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load and tokenize the corpus from multiple sources\n",
    "def load_and_tokenize_corpus():\n",
    "    # Load Gutenberg corpus\n",
    "    nltk.download('gutenberg')\n",
    "    from nltk.corpus import gutenberg\n",
    "    gutenberg_corpus = gutenberg.raw()\n",
    "\n",
    "    # Load Wikipedia articles\n",
    "    wiki_dataset = load_dataset('wikipedia', '20220301.en', split='train[:1%]')  # Use a subset for demonstration\n",
    "    wiki_corpus = \" \".join(wiki_dataset['text'])\n",
    "\n",
    "    # Combine corpora\n",
    "    combined_corpus = gutenberg_corpus + \" \" + wiki_corpus\n",
    "\n",
    "    # Tokenize corpus\n",
    "    tokens = word_tokenize(combined_corpus.lower())\n",
    "    return tokens\n",
    "\n",
    "tokens = load_and_tokenize_corpus()\n",
    "\n",
    "# Build vocabulary and mappings\n",
    "vocab = Counter(tokens)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Convert tokens to indices\n",
    "data = [word_to_idx[word] for word in tokens if word in word_to_idx]\n",
    "\n",
    "def create_batches(data, batch_size, seq_length):\n",
    "    num_batches = len(data) // (batch_size * seq_length)\n",
    "    data = data[:num_batches * batch_size * seq_length]\n",
    "    inputs = np.array(data).reshape((batch_size, -1))\n",
    "    targets = np.copy(inputs)\n",
    "    targets[:, :-1], targets[:, -1] = inputs[:, 1:], inputs[:, 0]\n",
    "    return inputs, targets\n",
    "\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "inputs, targets = create_batches(data, batch_size, seq_length)\n",
    "\n",
    "class miniLlama(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos):\n",
    "        super(miniLlama, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos = pos\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward), num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward), num_decoder_layers)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        src = self.encoder(src)\n",
    "        tgt = self.decoder(tgt, src)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        tgt = self.fc(tgt)\n",
    "        return tgt\n",
    "    \n",
    "    def generate(self, src):\n",
    "        src = self.embedding(src)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.encoder(src)\n",
    "        tgt = torch.zeros((1, src.shape[1], self.d_model)).to(src.device)\n",
    "        for i in range(self.max_seq_length):\n",
    "            tgt = self.decoder(tgt, src)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = self.fc(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt = torch.argmax(tgt, dim=2)\n",
    "            if torch.all(tgt[-1] == self.pos):\n",
    "                break\n",
    "        return tgt\n",
    "\n",
    "# Training loop\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    vocab_size = len(vocab)\n",
    "    d_model = 128\n",
    "    nhead = 4\n",
    "    num_encoder_layers = 2\n",
    "    num_decoder_layers = 2\n",
    "    dim_feedforward = 512\n",
    "    max_seq_length = seq_length\n",
    "    pos = torch.tensor([vocab_size-1])\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = miniLlama(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(range(0, inputs.shape[1] - seq_length, seq_length), desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        for i in progress_bar:\n",
    "            input_batch = torch.tensor(inputs[:, i:i+seq_length], dtype=torch.long)\n",
    "            target_batch = torch.tensor(targets[:, i:i+seq_length], dtype=torch.long)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_batch, target_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.view(-1, vocab_size), target_batch.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / (inputs.shape[1] // seq_length)}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'mini_llama_model_new.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
